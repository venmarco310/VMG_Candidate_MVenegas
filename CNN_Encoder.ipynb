{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indoor-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "asian-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioFaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BioFaceNet, self).__init__()\n",
    "        self.skip = []\n",
    "        inputChannels = 3\n",
    "        nfeatures = np.array([32,64,128,256,512])\n",
    "        kernelSize = 3\n",
    "        pad = 1\n",
    "        nclasses = 4\n",
    "        LightVectorSize = 15\n",
    "        bSize = 2\n",
    "        dims = LightVectorSize+bSize\n",
    "        \n",
    "        ################################# Encoding #####################################\n",
    "        self.conv1dn = nn.Conv2d(inputChannels,nfeatures[0], kernel_size = 3,padding = pad,bias=False)\n",
    "        self.bnorm1 = nn.BatchNorm2d(num_features = nfeatures[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2dn = nn.Conv2d(nfeatures[0],nfeatures[0], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3dn = nn.Conv2d(nfeatures[0],nfeatures[1], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        self.bnorm2 = nn.BatchNorm2d(num_features = nfeatures[1])\n",
    "        self.conv4dn = nn.Conv2d(nfeatures[1],nfeatures[1], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        \n",
    "        self.conv5dn = nn.Conv2d(nfeatures[1],nfeatures[2], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        self.bnorm3 = nn.BatchNorm2d(num_features = nfeatures[2])\n",
    "        self.conv6dn = nn.Conv2d(nfeatures[2],nfeatures[2], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        \n",
    "        self.conv7dn = nn.Conv2d(nfeatures[2],nfeatures[3], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        self.bnorm4 = nn.BatchNorm2d(num_features = nfeatures[3])\n",
    "        self.conv8dn = nn.Conv2d(nfeatures[3],nfeatures[3], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        \n",
    "        self.conv9dn = nn.Conv2d(nfeatures[3],nfeatures[4], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        self.bnorm5 = nn.BatchNorm2d(num_features = nfeatures[4])\n",
    "        self.conv10dn = nn.Conv2d(nfeatures[4],nfeatures[4], kernel_size = (3,3),padding = pad,bias=False)\n",
    "        ##############################################################################\n",
    "        \n",
    "        ################################# Decoding #####################################\n",
    "        self.upsample1 = nn.Upsample(size = (27,22))\n",
    "        self.upsample2 = nn.Upsample(size = (54,44))\n",
    "        self.upsample3 = nn.Upsample(size = (109,89))\n",
    "        self.upsample4 = nn.Upsample(size = (218,178))\n",
    "        self.upsamp = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv1up = nn.Conv2d(nfeatures[4]+nfeatures[3],nfeatures[3], kernel_size=(3,3),padding = pad,bias=False)\n",
    "        self.conv2up = nn.Conv2d(nfeatures[3]+nfeatures[2],nfeatures[2], kernel_size=(3,3),padding = pad,bias=False)\n",
    "        self.conv3up = nn.Conv2d(nfeatures[2]+nfeatures[1],nfeatures[1], kernel_size=(3,3),padding = pad,bias=False)\n",
    "        self.conv4up = nn.Conv2d(nfeatures[1]+nfeatures[0],nfeatures[0], kernel_size=(3,3),padding = pad,bias=False)\n",
    "        self.convFD = nn.Conv2d(nfeatures[0],1, kernel_size= (3,3),padding = pad,bias=False)\n",
    "        ##############################################################################\n",
    "        \n",
    "        ################################# FCN #####################################\n",
    "        self.convFCN1 = nn.Conv2d(in_channels=nfeatures[4], out_channels=nfeatures[4], kernel_size = 4)\n",
    "        self.bnormFCN1 = nn.BatchNorm2d(num_features=nfeatures[4])\n",
    "        self.reluFCN1 = nn.ReLU(inplace=True)\n",
    "        self.convFCN2 = nn.Conv2d(in_channels=nfeatures[4], out_channels=nfeatures[4], kernel_size = 1)\n",
    "        self.bnormFCN2 = nn.BatchNorm2d(num_features=nfeatures[4])\n",
    "        self.reluFCN2 = nn.ReLU(inplace=True)\n",
    "        ##############################################################################\n",
    "        \n",
    "        self.pred = nn.Conv2d(in_channels = nfeatures[4],out_channels = 17, kernel_size = 2,bias=False)\n",
    "        self.predLinear = nn.Linear(in_features = 17,out_features=17)\n",
    "        \n",
    "        ####scaling####\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Assuming that the doubleconv will always be true\n",
    "        \n",
    "        x = x.permute(0,3,1,2) # only use this when using original imgData\n",
    "        ########Encoding################\n",
    "        x = self.conv1dn(x) #iteration 1\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm1(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv2dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm1(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv2dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm1(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        encode1 = self.relu(x)\n",
    "        #print(\"Eshape: \"+str(encode1.shape))\n",
    "        #self.skip.append(x)\n",
    "        x= self.pool(encode1)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        \n",
    "        x = self.conv3dn(x) #iteration 2\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm2(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv4dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm2(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv4dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm2(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        encode2 = self.relu(x)\n",
    "        #print(\"Eshape: \"+str(encode2.shape))\n",
    "        #self.skip.append(x)\n",
    "        x =self.pool(encode2)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        \n",
    "        x = self.conv5dn(x) #iteration 3\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm3(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv6dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm3(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv6dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm3(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        encode3 = self.relu(x)\n",
    "        #print(\"Eshape: \"+str(encode3.shape))\n",
    "        #self.skip.append(x)\n",
    "        x =self.pool(encode3)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        \n",
    "        x = self.conv7dn(x) #iteration 4\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm4(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv8dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm4(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv8dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm4(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        encode4 = self.relu(x)\n",
    "        #print(\"Eshape: \"+str(encode4.shape))\n",
    "        #self.skip.append(x)\n",
    "        x =self.pool(encode4)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        \n",
    "        x = self.conv9dn(x) #iteration 5\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm5(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv10dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm5(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.relu(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.conv10dn(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        x = self.bnorm5(x)\n",
    "        #print(\"shape: \"+str(x.shape))\n",
    "        encode5 = self.relu(x)\n",
    "        #print(\"Eshape: \"+str(encode5.shape))\n",
    "        #self.skip.append(x)\n",
    "        y =self.pool(encode5)\n",
    "        #print(\"shape: \"+str(y.shape))\n",
    "        ################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        ########Decoding################\n",
    "        for c in range(1,5):\n",
    "            x = self.upsample1(y) ### iteration 1\n",
    "            #print(\"----------------Decoding----------------\")\n",
    "            #print(\"X shape: \"+str(x.shape))\n",
    "            #print(\"Encode5 shape: \"+str(encode5.shape))\n",
    "            x = torch.cat((x,encode4),1)\n",
    "            x = self.conv1up(x)\n",
    "            x = self.bnorm4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv8dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm4(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv8dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm4(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.upsample2(x) ### iteration 2\n",
    "            x = torch.cat((x,encode3),1)\n",
    "            x = self.conv2up(x)\n",
    "            x = self.bnorm3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv6dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm3(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv6dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm3(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.upsample3(x) ### iteration 3\n",
    "            x = torch.cat((x,encode2),1)\n",
    "            x = self.conv3up(x)\n",
    "            x = self.bnorm2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv4dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv4dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm2(x)\n",
    "            x = self.relu(x)\n",
    "\n",
    "            x = self.upsample4(x) ### iteration 4\n",
    "            x = torch.cat((x,encode1),1)\n",
    "            x = self.conv4up(x)\n",
    "            x = self.bnorm1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv2dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv2dn(x) #not a down conv., this has the same param for a double conv.\n",
    "            x = self.bnorm1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.convFD(x)\n",
    "            \n",
    "            if c==1:\n",
    "                z = x\n",
    "            else:\n",
    "                z = torch.cat((z,x),1)\n",
    "        x = z\n",
    "        ################################\n",
    "        \n",
    "        ######## FCN ################\n",
    "        fcn1 = self.convFCN1(y)\n",
    "        fcn1 = self.bnormFCN1(fcn1)\n",
    "        fcn1 = self.reluFCN1(fcn1)\n",
    "        fcn2 = self.convFCN2(fcn1)\n",
    "        fcn2 = self.bnormFCN2(fcn2)\n",
    "        fcn2 = self.reluFCN2(fcn2)\n",
    "        #############################\n",
    "        \n",
    "        predictions = self.pred(fcn2)\n",
    "        #predictions = self.predLinear(predictions)\n",
    "        \n",
    "        \n",
    "        #print(\"prediction shape: \",predictions.shape)\n",
    "        lightingParameters = predictions[:,0:LightVectorSize,0,0]\n",
    "        batch = predictions[:,15:17,0,0]\n",
    "        #print(batch.shape)\n",
    "        #nbatch = batch.shape[0]*batch.shape[2]*batch.shape[3]\n",
    "        b = batch\n",
    "        fmel = x[:,0,:,:]\n",
    "        fblood = x[:,1,:,:]\n",
    "        shading = x[:,2,:,:]\n",
    "        specmask = x[:,3,:,:]\n",
    "\n",
    "        return lightingParameters,b,fmel,fblood,shading,specmask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-visit",
   "metadata": {},
   "source": [
    "class ScalingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingNet, self).__init__()\n",
    "        self.skip = []\n",
    "        inputChannels = 3\n",
    "        nfeatures = np.array([32,64,128,256,512])\n",
    "        kernelSize = 3\n",
    "        pad = 1\n",
    "        nclasses = 4\n",
    "        LightVectorSize = 15\n",
    "        bSize = 2\n",
    "        dims = LightVectorSize+bSize\n",
    "        \n",
    "                ####scaling####\n",
    "        self.soft = nn.Softmax()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(lightingParameters,b,fmel,fblood,shading,specmask):\n",
    "        numBatch = 17\n",
    "        lightweights = self.softmax(lightingparameters[:,0:15,:,:])\n",
    "        weightA = lightingWeights[:,0,:,:,] \n",
    "        weightD = lightingWeights[:,1,:,:,]\n",
    "        Fweights = lightingWeights[:,2:14,:,:,]\n",
    "        CCT =  lightingparameters[:,:,14,:]\n",
    "        CCT = ((22-1) / (1 + torch.exp(-1*CCT))) + 1\n",
    "        b = 6*self.sig(b) - 3\n",
    "        Bgrid = b.reshape(2,1,1,numBatch)\n",
    "        Bgrid = Bgrid/3\n",
    "        fmel = 2*self.sig(fmel) - 1\n",
    "        fblood = 2*self.sig(fmel) - 1\n",
    "        shading = torch.exp(shading)\n",
    "        specmask = torch.exp(shading)\n",
    "        return weightA,weightD,CCT,Fweights,b,BGrid,fmel,fblood,Shading,specmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-attention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
